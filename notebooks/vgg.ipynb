{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(\"/projects/computer-vision/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-01 16:53:30.768071: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras as K\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-01 16:53:35.813668: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:26:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-12-01 16:53:35.828612: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:26:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-12-01 16:53:35.828819: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:26:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n"
     ]
    }
   ],
   "source": [
    "gpus = tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if gpus:\n",
    "#     for gpu in gpus:\n",
    "#         tf.config.experimental.set_virtual_device_configuration(gpu,[tf.config.experimental.VirtualDeviceConfiguration(memory_limit=4096)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    for gpu in gpus:\n",
    "       tf.config.experimental.set_memory_growth(gpu,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: CPU random generator seem to be failing, disabling hardware random number generation\n",
      "WARNING: RDRND generated: 0xffffffff 0xffffffff 0xffffffff 0xffffffff\n"
     ]
    }
   ],
   "source": [
    "from utils import cv_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using matplotlib backend: <object object at 0x7f424e1447b0>\n"
     ]
    }
   ],
   "source": [
    "%matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy as scp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading data\n",
    "# Loading metadata\n",
    "wiki_metadata_df = scp.io.loadmat('/datasets/wiki/wiki.mat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_columns = wiki_metadata_df['wiki'].dtype.fields.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_df = wiki_metadata_df['wiki'][0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_dict = { col:metadata_df[i] for col, i in zip(metadata_columns, range(len(metadata_columns))) }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 62328),\n",
       " (1, 62328),\n",
       " (1, 62328),\n",
       " (1, 62328),\n",
       " (1, 62328),\n",
       " (1, 62328),\n",
       " (1, 62328),\n",
       " (1, 62328)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[ metadata_dict[c].shape for c in metadata_dict ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_df = pd.DataFrame({ c:metadata_dict[c][0].tolist() for c in metadata_dict })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dob</th>\n",
       "      <th>photo_taken</th>\n",
       "      <th>full_path</th>\n",
       "      <th>gender</th>\n",
       "      <th>name</th>\n",
       "      <th>face_location</th>\n",
       "      <th>face_score</th>\n",
       "      <th>second_face_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>723671</td>\n",
       "      <td>2009</td>\n",
       "      <td>[17/10000217_1981-05-05_2009.jpg]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[Sami Jauhojärvi]</td>\n",
       "      <td>[[111.29109473290997, 111.29109473290997, 252....</td>\n",
       "      <td>4.300962</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>703186</td>\n",
       "      <td>1964</td>\n",
       "      <td>[48/10000548_1925-04-04_1964.jpg]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[Dettmar Cramer]</td>\n",
       "      <td>[[252.48330229530742, 126.68165114765371, 354....</td>\n",
       "      <td>2.645639</td>\n",
       "      <td>1.949248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>711677</td>\n",
       "      <td>2008</td>\n",
       "      <td>[12/100012_1948-07-03_2008.jpg]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[Marc Okrand]</td>\n",
       "      <td>[[113.52, 169.83999999999997, 366.08, 422.4]]</td>\n",
       "      <td>4.329329</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>705061</td>\n",
       "      <td>1961</td>\n",
       "      <td>[65/10001965_1930-05-23_1961.jpg]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[Aleksandar Matanović]</td>\n",
       "      <td>[[1, 1, 634, 440]]</td>\n",
       "      <td>-inf</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>720044</td>\n",
       "      <td>2012</td>\n",
       "      <td>[16/10002116_1971-05-31_2012.jpg]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[Diana Damrau]</td>\n",
       "      <td>[[171.61031405173117, 75.57451239763239, 266.7...</td>\n",
       "      <td>3.408442</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      dob  photo_taken                          full_path  gender  \\\n",
       "0  723671         2009  [17/10000217_1981-05-05_2009.jpg]     1.0   \n",
       "1  703186         1964  [48/10000548_1925-04-04_1964.jpg]     1.0   \n",
       "2  711677         2008    [12/100012_1948-07-03_2008.jpg]     1.0   \n",
       "3  705061         1961  [65/10001965_1930-05-23_1961.jpg]     1.0   \n",
       "4  720044         2012  [16/10002116_1971-05-31_2012.jpg]     0.0   \n",
       "\n",
       "                     name                                      face_location  \\\n",
       "0       [Sami Jauhojärvi]  [[111.29109473290997, 111.29109473290997, 252....   \n",
       "1        [Dettmar Cramer]  [[252.48330229530742, 126.68165114765371, 354....   \n",
       "2           [Marc Okrand]      [[113.52, 169.83999999999997, 366.08, 422.4]]   \n",
       "3  [Aleksandar Matanović]                                 [[1, 1, 634, 440]]   \n",
       "4          [Diana Damrau]  [[171.61031405173117, 75.57451239763239, 266.7...   \n",
       "\n",
       "   face_score  second_face_score  \n",
       "0    4.300962                NaN  \n",
       "1    2.645639           1.949248  \n",
       "2    4.329329                NaN  \n",
       "3        -inf                NaN  \n",
       "4    3.408442                NaN  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_df['full_path'] = metadata_df['full_path'].apply(lambda x: x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "YEAR_ZERO_UNIX_EPOCH = 719529 # epoch of 1970-01-01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/envs/cv-env/lib/python3.8/site-packages/numpy/lib/function_base.py:4527: RuntimeWarning: invalid value encountered in subtract\n",
      "  diff_b_a = subtract(b, a)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>dob</th>\n",
       "      <td>62328.0</td>\n",
       "      <td>7.166810e+05</td>\n",
       "      <td>10832.627367</td>\n",
       "      <td>4077.000000</td>\n",
       "      <td>710673.000000</td>\n",
       "      <td>719269.000000</td>\n",
       "      <td>724702.250000</td>\n",
       "      <td>736011.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>photo_taken</th>\n",
       "      <td>62328.0</td>\n",
       "      <td>1.998766e+03</td>\n",
       "      <td>20.907937</td>\n",
       "      <td>1940.000000</td>\n",
       "      <td>1992.750000</td>\n",
       "      <td>2009.000000</td>\n",
       "      <td>2012.000000</td>\n",
       "      <td>2015.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gender</th>\n",
       "      <td>59685.0</td>\n",
       "      <td>7.885231e-01</td>\n",
       "      <td>0.408359</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>face_score</th>\n",
       "      <td>62328.0</td>\n",
       "      <td>-inf</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-inf</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.463949</td>\n",
       "      <td>3.760014</td>\n",
       "      <td>7.081268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>second_face_score</th>\n",
       "      <td>4096.0</td>\n",
       "      <td>2.009050e+00</td>\n",
       "      <td>0.981277</td>\n",
       "      <td>0.731419</td>\n",
       "      <td>1.164925</td>\n",
       "      <td>1.839065</td>\n",
       "      <td>2.655757</td>\n",
       "      <td>5.463147</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     count          mean           std          min  \\\n",
       "dob                62328.0  7.166810e+05  10832.627367  4077.000000   \n",
       "photo_taken        62328.0  1.998766e+03     20.907937  1940.000000   \n",
       "gender             59685.0  7.885231e-01      0.408359     0.000000   \n",
       "face_score         62328.0          -inf           NaN         -inf   \n",
       "second_face_score   4096.0  2.009050e+00      0.981277     0.731419   \n",
       "\n",
       "                             25%            50%            75%            max  \n",
       "dob                710673.000000  719269.000000  724702.250000  736011.000000  \n",
       "photo_taken          1992.750000    2009.000000    2012.000000    2015.000000  \n",
       "gender                  1.000000       1.000000       1.000000       1.000000  \n",
       "face_score                   NaN       2.463949       3.760014       7.081268  \n",
       "second_face_score       1.164925       1.839065       2.655757       5.463147  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata_df.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((62328,), (62328,))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata_df['dob'].dropna().shape, metadata_df['dob'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0    47063\n",
       "0.0    12622\n",
       "Name: gender, dtype: int64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata_df['gender'].value_counts()\n",
    "# Oh only binary gender!? HOHO !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((59685,), (62328,))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata_df['gender'].dropna().shape, metadata_df['gender'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_df = metadata_df.dropna(subset=['gender'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_df = metadata_df[metadata_df['face_score'].apply(np.isfinite)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_df['name'] = metadata_df['name'].apply(lambda x: x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dob</th>\n",
       "      <th>photo_taken</th>\n",
       "      <th>full_path</th>\n",
       "      <th>gender</th>\n",
       "      <th>name</th>\n",
       "      <th>face_location</th>\n",
       "      <th>face_score</th>\n",
       "      <th>second_face_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>723671</td>\n",
       "      <td>2009</td>\n",
       "      <td>17/10000217_1981-05-05_2009.jpg</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Sami Jauhojärvi</td>\n",
       "      <td>[[111.29109473290997, 111.29109473290997, 252....</td>\n",
       "      <td>4.300962</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>703186</td>\n",
       "      <td>1964</td>\n",
       "      <td>48/10000548_1925-04-04_1964.jpg</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Dettmar Cramer</td>\n",
       "      <td>[[252.48330229530742, 126.68165114765371, 354....</td>\n",
       "      <td>2.645639</td>\n",
       "      <td>1.949248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>711677</td>\n",
       "      <td>2008</td>\n",
       "      <td>12/100012_1948-07-03_2008.jpg</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Marc Okrand</td>\n",
       "      <td>[[113.52, 169.83999999999997, 366.08, 422.4]]</td>\n",
       "      <td>4.329329</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>720044</td>\n",
       "      <td>2012</td>\n",
       "      <td>16/10002116_1971-05-31_2012.jpg</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Diana Damrau</td>\n",
       "      <td>[[171.61031405173117, 75.57451239763239, 266.7...</td>\n",
       "      <td>3.408442</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>716189</td>\n",
       "      <td>2012</td>\n",
       "      <td>02/10002702_1960-11-09_2012.jpg</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Krista Tippett</td>\n",
       "      <td>[[274.76563240288175, 57.7700900839337, 376.88...</td>\n",
       "      <td>4.748056</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      dob  photo_taken                        full_path  gender  \\\n",
       "0  723671         2009  17/10000217_1981-05-05_2009.jpg     1.0   \n",
       "1  703186         1964  48/10000548_1925-04-04_1964.jpg     1.0   \n",
       "2  711677         2008    12/100012_1948-07-03_2008.jpg     1.0   \n",
       "4  720044         2012  16/10002116_1971-05-31_2012.jpg     0.0   \n",
       "5  716189         2012  02/10002702_1960-11-09_2012.jpg     0.0   \n",
       "\n",
       "              name                                      face_location  \\\n",
       "0  Sami Jauhojärvi  [[111.29109473290997, 111.29109473290997, 252....   \n",
       "1   Dettmar Cramer  [[252.48330229530742, 126.68165114765371, 354....   \n",
       "2      Marc Okrand      [[113.52, 169.83999999999997, 366.08, 422.4]]   \n",
       "4     Diana Damrau  [[171.61031405173117, 75.57451239763239, 266.7...   \n",
       "5   Krista Tippett  [[274.76563240288175, 57.7700900839337, 376.88...   \n",
       "\n",
       "   face_score  second_face_score  \n",
       "0    4.300962                NaN  \n",
       "1    2.645639           1.949248  \n",
       "2    4.329329                NaN  \n",
       "4    3.408442                NaN  \n",
       "5    4.748056                NaN  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot: >"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata_df['gender'].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will sample 10K females and 10k males images to balance out the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def from_matlab_to_datetime(d):\n",
    "    try:\n",
    "        return pd.to_datetime(d-YEAR_ZERO_UNIX_EPOCH, unit='D')\n",
    "    except:\n",
    "        None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_df['date_of_birth'] = metadata_df['dob'].apply(from_matlab_to_datetime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0   1981-05-05\n",
       "Name: dob, dtype: datetime64[ns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata_df['dob'].iloc[:1].apply(from_matlab_to_datetime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot: >"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata_df['date_of_birth'].hist(bins=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime as dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_df['year_of_birth'] = metadata_df['date_of_birth'].apply(lambda x : x.year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_df['age'] = metadata_df['photo_taken'] - metadata_df['year_of_birth']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot: >"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata_df['age'].hist(bins=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dob</th>\n",
       "      <th>photo_taken</th>\n",
       "      <th>full_path</th>\n",
       "      <th>gender</th>\n",
       "      <th>name</th>\n",
       "      <th>face_location</th>\n",
       "      <th>face_score</th>\n",
       "      <th>second_face_score</th>\n",
       "      <th>date_of_birth</th>\n",
       "      <th>year_of_birth</th>\n",
       "      <th>age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>723671</td>\n",
       "      <td>2009</td>\n",
       "      <td>17/10000217_1981-05-05_2009.jpg</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Sami Jauhojärvi</td>\n",
       "      <td>[[111.29109473290997, 111.29109473290997, 252....</td>\n",
       "      <td>4.300962</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1981-05-05</td>\n",
       "      <td>1981.0</td>\n",
       "      <td>28.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>703186</td>\n",
       "      <td>1964</td>\n",
       "      <td>48/10000548_1925-04-04_1964.jpg</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Dettmar Cramer</td>\n",
       "      <td>[[252.48330229530742, 126.68165114765371, 354....</td>\n",
       "      <td>2.645639</td>\n",
       "      <td>1.949248</td>\n",
       "      <td>1925-04-04</td>\n",
       "      <td>1925.0</td>\n",
       "      <td>39.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>711677</td>\n",
       "      <td>2008</td>\n",
       "      <td>12/100012_1948-07-03_2008.jpg</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Marc Okrand</td>\n",
       "      <td>[[113.52, 169.83999999999997, 366.08, 422.4]]</td>\n",
       "      <td>4.329329</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1948-07-03</td>\n",
       "      <td>1948.0</td>\n",
       "      <td>60.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>720044</td>\n",
       "      <td>2012</td>\n",
       "      <td>16/10002116_1971-05-31_2012.jpg</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Diana Damrau</td>\n",
       "      <td>[[171.61031405173117, 75.57451239763239, 266.7...</td>\n",
       "      <td>3.408442</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1971-05-31</td>\n",
       "      <td>1971.0</td>\n",
       "      <td>41.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>716189</td>\n",
       "      <td>2012</td>\n",
       "      <td>02/10002702_1960-11-09_2012.jpg</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Krista Tippett</td>\n",
       "      <td>[[274.76563240288175, 57.7700900839337, 376.88...</td>\n",
       "      <td>4.748056</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1960-11-09</td>\n",
       "      <td>1960.0</td>\n",
       "      <td>52.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      dob  photo_taken                        full_path  gender  \\\n",
       "0  723671         2009  17/10000217_1981-05-05_2009.jpg     1.0   \n",
       "1  703186         1964  48/10000548_1925-04-04_1964.jpg     1.0   \n",
       "2  711677         2008    12/100012_1948-07-03_2008.jpg     1.0   \n",
       "4  720044         2012  16/10002116_1971-05-31_2012.jpg     0.0   \n",
       "5  716189         2012  02/10002702_1960-11-09_2012.jpg     0.0   \n",
       "\n",
       "              name                                      face_location  \\\n",
       "0  Sami Jauhojärvi  [[111.29109473290997, 111.29109473290997, 252....   \n",
       "1   Dettmar Cramer  [[252.48330229530742, 126.68165114765371, 354....   \n",
       "2      Marc Okrand      [[113.52, 169.83999999999997, 366.08, 422.4]]   \n",
       "4     Diana Damrau  [[171.61031405173117, 75.57451239763239, 266.7...   \n",
       "5   Krista Tippett  [[274.76563240288175, 57.7700900839337, 376.88...   \n",
       "\n",
       "   face_score  second_face_score date_of_birth  year_of_birth   age  \n",
       "0    4.300962                NaN    1981-05-05         1981.0  28.0  \n",
       "1    2.645639           1.949248    1925-04-04         1925.0  39.0  \n",
       "2    4.329329                NaN    1948-07-03         1948.0  60.0  \n",
       "4    3.408442                NaN    1971-05-31         1971.0  41.0  \n",
       "5    4.748056                NaN    1960-11-09         1960.0  52.0  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 as cv\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLE_SIZE = 8000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((43452,), (43452,), (43452,))"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata_df['full_path'].dropna().shape,metadata_df['full_path'].shape, metadata_df['full_path'].drop_duplicates().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_df.index = metadata_df['full_path']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "female_index = np.random.choice(metadata_df[metadata_df['gender'] == 0].index, SAMPLE_SIZE, replace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "male_index = np.random.choice(metadata_df[metadata_df['gender'] == 1].index, SAMPLE_SIZE, replace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.concat([\n",
    "    metadata_df.loc[female_index],\n",
    "    metadata_df.loc[male_index]\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16000, 11)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dob</th>\n",
       "      <th>photo_taken</th>\n",
       "      <th>full_path</th>\n",
       "      <th>gender</th>\n",
       "      <th>name</th>\n",
       "      <th>face_location</th>\n",
       "      <th>face_score</th>\n",
       "      <th>second_face_score</th>\n",
       "      <th>date_of_birth</th>\n",
       "      <th>year_of_birth</th>\n",
       "      <th>age</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>full_path</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>73/29874973_1962-11-07_2010.jpg</th>\n",
       "      <td>716917</td>\n",
       "      <td>2010</td>\n",
       "      <td>73/29874973_1962-11-07_2010.jpg</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Hilary Thayer Hamann</td>\n",
       "      <td>[[185.58576137539876, 53.441646107256794, 303....</td>\n",
       "      <td>3.748159</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1962-11-07</td>\n",
       "      <td>1962.0</td>\n",
       "      <td>48.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78/2172078_1967-10-13_2011.jpg</th>\n",
       "      <td>718718</td>\n",
       "      <td>2011</td>\n",
       "      <td>78/2172078_1967-10-13_2011.jpg</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Kate Walsh</td>\n",
       "      <td>[[98.93881544967539, 123.45351931209424, 318.6...</td>\n",
       "      <td>4.744765</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1967-10-13</td>\n",
       "      <td>1967.0</td>\n",
       "      <td>44.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58/33029258_1976-06-13_2012.jpg</th>\n",
       "      <td>721884</td>\n",
       "      <td>2012</td>\n",
       "      <td>58/33029258_1976-06-13_2012.jpg</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Sarah Stewart</td>\n",
       "      <td>[[140.26488870749222, 80.5285078328527, 318.59...</td>\n",
       "      <td>4.549433</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1976-06-13</td>\n",
       "      <td>1976.0</td>\n",
       "      <td>36.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70/12939770_1984-09-20_2007.jpg</th>\n",
       "      <td>724905</td>\n",
       "      <td>2007</td>\n",
       "      <td>70/12939770_1984-09-20_2007.jpg</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Holly Weber</td>\n",
       "      <td>[[32.89483690955212, 81.3670922738803, 177.731...</td>\n",
       "      <td>4.654479</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1984-09-20</td>\n",
       "      <td>1984.0</td>\n",
       "      <td>23.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97/225497_1934-07-22_1961.jpg</th>\n",
       "      <td>706582</td>\n",
       "      <td>1961</td>\n",
       "      <td>97/225497_1934-07-22_1961.jpg</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Louise Fletcher</td>\n",
       "      <td>[[115.83273736172191, 66.54756420669824, 262.8...</td>\n",
       "      <td>4.955657</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1934-07-22</td>\n",
       "      <td>1934.0</td>\n",
       "      <td>27.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    dob  photo_taken  \\\n",
       "full_path                                              \n",
       "73/29874973_1962-11-07_2010.jpg  716917         2010   \n",
       "78/2172078_1967-10-13_2011.jpg   718718         2011   \n",
       "58/33029258_1976-06-13_2012.jpg  721884         2012   \n",
       "70/12939770_1984-09-20_2007.jpg  724905         2007   \n",
       "97/225497_1934-07-22_1961.jpg    706582         1961   \n",
       "\n",
       "                                                       full_path  gender  \\\n",
       "full_path                                                                  \n",
       "73/29874973_1962-11-07_2010.jpg  73/29874973_1962-11-07_2010.jpg     0.0   \n",
       "78/2172078_1967-10-13_2011.jpg    78/2172078_1967-10-13_2011.jpg     0.0   \n",
       "58/33029258_1976-06-13_2012.jpg  58/33029258_1976-06-13_2012.jpg     0.0   \n",
       "70/12939770_1984-09-20_2007.jpg  70/12939770_1984-09-20_2007.jpg     0.0   \n",
       "97/225497_1934-07-22_1961.jpg      97/225497_1934-07-22_1961.jpg     0.0   \n",
       "\n",
       "                                                 name  \\\n",
       "full_path                                               \n",
       "73/29874973_1962-11-07_2010.jpg  Hilary Thayer Hamann   \n",
       "78/2172078_1967-10-13_2011.jpg             Kate Walsh   \n",
       "58/33029258_1976-06-13_2012.jpg         Sarah Stewart   \n",
       "70/12939770_1984-09-20_2007.jpg           Holly Weber   \n",
       "97/225497_1934-07-22_1961.jpg         Louise Fletcher   \n",
       "\n",
       "                                                                     face_location  \\\n",
       "full_path                                                                            \n",
       "73/29874973_1962-11-07_2010.jpg  [[185.58576137539876, 53.441646107256794, 303....   \n",
       "78/2172078_1967-10-13_2011.jpg   [[98.93881544967539, 123.45351931209424, 318.6...   \n",
       "58/33029258_1976-06-13_2012.jpg  [[140.26488870749222, 80.5285078328527, 318.59...   \n",
       "70/12939770_1984-09-20_2007.jpg  [[32.89483690955212, 81.3670922738803, 177.731...   \n",
       "97/225497_1934-07-22_1961.jpg    [[115.83273736172191, 66.54756420669824, 262.8...   \n",
       "\n",
       "                                 face_score  second_face_score date_of_birth  \\\n",
       "full_path                                                                      \n",
       "73/29874973_1962-11-07_2010.jpg    3.748159                NaN    1962-11-07   \n",
       "78/2172078_1967-10-13_2011.jpg     4.744765                NaN    1967-10-13   \n",
       "58/33029258_1976-06-13_2012.jpg    4.549433                NaN    1976-06-13   \n",
       "70/12939770_1984-09-20_2007.jpg    4.654479                NaN    1984-09-20   \n",
       "97/225497_1934-07-22_1961.jpg      4.955657                NaN    1934-07-22   \n",
       "\n",
       "                                 year_of_birth   age  \n",
       "full_path                                             \n",
       "73/29874973_1962-11-07_2010.jpg         1962.0  48.0  \n",
       "78/2172078_1967-10-13_2011.jpg          1967.0  44.0  \n",
       "58/33029258_1976-06-13_2012.jpg         1976.0  36.0  \n",
       "70/12939770_1984-09-20_2007.jpg         1984.0  23.0  \n",
       "97/225497_1934-07-22_1961.jpg           1934.0  27.0  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(dataset.shape)\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (2, 28, 28, 3)\n",
    "x = np.arange(np.prod(input_shape)).reshape(input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 28, 28, 3)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [ (t, b), (l, r) for t, l, b, r in tdataset['face_location'][0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset['crop_coords'] = dataset['face_location'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "jpg    16000\n",
       "Name: full_path, dtype: int64"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['full_path'].apply(lambda x: x[-3:]).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We only have image of format `jpg`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROOT_PATH = \"/datasets/wiki/images/\"\n",
    "ROOT_PATH = \"/datasets/wiki_crop/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# images = tf.keras.preprocessing.image_dataset_from_directory(ROOT_PATH, interpolation='gaussian')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_HEIGHT, IMG_WIDTH = 256, 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-01 16:53:46.206367: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-12-01 16:53:46.209187: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:26:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-12-01 16:53:46.209578: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:26:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-12-01 16:53:46.209851: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:26:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-12-01 16:53:47.370933: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:26:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-12-01 16:53:47.371726: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:26:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-12-01 16:53:47.371743: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1700] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2022-12-01 16:53:47.371965: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:26:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-12-01 16:53:47.372297: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 5898 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2080, pci bus id: 0000:26:00.0, compute capability: 7.5\n"
     ]
    }
   ],
   "source": [
    "tf_dataset_df = tf.data.Dataset.from_tensor_slices(\n",
    "    (\n",
    "        tf.cast(dataset['full_path'], tf.string),\n",
    "        # tf.cast(dataset['crop_coords'], tf.int32),\n",
    "        tf.cast(dataset['age'], tf.int32),\n",
    "        tf.cast(dataset['gender'], tf.int32)\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(b'73/29874973_1962-11-07_2010.jpg', shape=(), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "for f, a, d in tf_dataset_df.take(1):\n",
    "    print(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we use the path as key in this dataset\n",
    "def decode_img(img):\n",
    "    img = tf.io.decode_jpeg(img, channels=3)\n",
    "    return tf.image.resize(img, [IMG_HEIGHT, IMG_WIDTH])\n",
    "\n",
    "def preprocess_data(path, age, label):\n",
    "    img = tf.io.read_file(ROOT_PATH + path)\n",
    "    img = decode_img(img)\n",
    "    return img, age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_SIZE, VAL_SIZE, TEST_SIZE = 0.8, 0.1, 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_size = len(tf_dataset_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_dataset_df = tf_dataset_df.shuffle(buffer_size=df_size)\n",
    "train_df = tf_dataset_df.take(round(TRAIN_SIZE * df_size))\n",
    "test_df = tf_dataset_df.skip(round(TRAIN_SIZE * df_size))\n",
    "val_df = test_df.take(round(VAL_SIZE * df_size))\n",
    "test_df = test_df.skip(round(VAL_SIZE * df_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "STANDARD_BATCH_SIZE, STANDARD_PREFETCH_BUFFER = 5, 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(b'90/25619490_1935-12-04_1967.jpg', shape=(), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "for f, a, d in train_df.take(1):\n",
    "    print(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df.map(preprocess_data, num_parallel_calls=AUTOTUNE)\n",
    "test_df = test_df.map(preprocess_data, num_parallel_calls=AUTOTUNE)\n",
    "val_df = val_df.map(preprocess_data, num_parallel_calls=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "SHUFFLE_BUFFER_SIZE = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_batch_load(tf_df, buffer_size=SHUFFLE_BUFFER_SIZE, batch_size=STANDARD_BATCH_SIZE, prefetch_buffer=STANDARD_PREFETCH_BUFFER):\n",
    "    tf_df = tf_df.shuffle(buffer_size=buffer_size)\n",
    "    tf_df = tf_df.batch(batch_size)\n",
    "    tf_df = tf_df.prefetch(buffer_size=prefetch_buffer)\n",
    "    return tf_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = prepare_batch_load(train_df)\n",
    "val_df = prepare_batch_load(val_df)\n",
    "test_df = prepare_batch_load(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleConvModel(K.Model):\n",
    "    def __init__(self, filters, kernel_size, strides, pool_size, activation, n_units=1):\n",
    "        super(SimpleConvModel, self).__init__()\n",
    "        self.filters = filters\n",
    "        self.kernel_size = kernel_size\n",
    "        self.norm = K.layers.Rescaling(1./255)\n",
    "        # Defining the layers\n",
    "        self.conv2D_0 = K.layers.Conv2D(\n",
    "            name='conv2D_0', \n",
    "            filters=self.filters, kernel_size=self.kernel_size, activation='relu', padding='same'\n",
    "        )\n",
    "\n",
    "        self.max_pool = K.layers.MaxPool2D(\n",
    "            name='max_pool_0',\n",
    "            pool_size=pool_size, strides=strides\n",
    "        )\n",
    "\n",
    "        self.flatten = K.layers.Flatten()\n",
    "\n",
    "        self.dense_0 = K.layers.Dense(units=32, activation='relu', name='dense_0')\n",
    "        self.predictor = K.layers.Dense(units=n_units, activation=K.activations.get(activation), name='age_predictor')\n",
    "        \n",
    "    def __call__(self, inputs, training=True):\n",
    "        x = self.norm(inputs)\n",
    "        x = self.conv2D_0(x)\n",
    "        x = self.max_pool(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.dense_0(x)\n",
    "        x = self.predictor(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_model = SimpleConvModel(filters=64, kernel_size=3, strides=1, pool_size=2, activation='linear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_model.compile(\n",
    "    optimizer='adam', loss='mse', metrics=['mape', 'mae']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [ c[0].numpy() for c in train_df.take(1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-01 16:54:13.951101: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:428] Loaded cuDNN version 8100\n",
      "2022-12-01 16:54:16.280287: I tensorflow/compiler/xla/service/service.cc:173] XLA service 0x1f7e4790 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2022-12-01 16:54:16.280358: I tensorflow/compiler/xla/service/service.cc:181]   StreamExecutor device (0): NVIDIA GeForce RTX 2080, Compute Capability 7.5\n",
      "2022-12-01 16:54:16.311346: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2022-12-01 16:54:16.582933: I tensorflow/compiler/jit/xla_compilation_cache.cc:477] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2560/2560 [==============================] - 83s 30ms/step - loss: 360287962136576.0000 - mape: 76.2408 - mae: 167806.7656 - val_loss: 1704.1460 - val_mape: 99.9568 - val_mae: 37.6592\n",
      "Epoch 2/10\n",
      " 495/2560 [====>.........................] - ETA: 58s - loss: 1704.3528 - mape: 99.9570 - mae: 37.5109"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[71], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m simple_model\u001b[39m.\u001b[39;49mfit(train_df, validation_data\u001b[39m=\u001b[39;49mval_df, epochs\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/engine/training.py:1656\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1654\u001b[0m logs \u001b[39m=\u001b[39m tmp_logs\n\u001b[1;32m   1655\u001b[0m end_step \u001b[39m=\u001b[39m step \u001b[39m+\u001b[39m data_handler\u001b[39m.\u001b[39mstep_increment\n\u001b[0;32m-> 1656\u001b[0m callbacks\u001b[39m.\u001b[39;49mon_train_batch_end(end_step, logs)\n\u001b[1;32m   1657\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstop_training:\n\u001b[1;32m   1658\u001b[0m     \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/callbacks.py:476\u001b[0m, in \u001b[0;36mCallbackList.on_train_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m    469\u001b[0m \u001b[39m\"\"\"Calls the `on_train_batch_end` methods of its callbacks.\u001b[39;00m\n\u001b[1;32m    470\u001b[0m \n\u001b[1;32m    471\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[1;32m    472\u001b[0m \u001b[39m    batch: Integer, index of batch within the current epoch.\u001b[39;00m\n\u001b[1;32m    473\u001b[0m \u001b[39m    logs: Dict. Aggregated metric results up until this batch.\u001b[39;00m\n\u001b[1;32m    474\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    475\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_should_call_train_batch_hooks:\n\u001b[0;32m--> 476\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_batch_hook(ModeKeys\u001b[39m.\u001b[39;49mTRAIN, \u001b[39m\"\u001b[39;49m\u001b[39mend\u001b[39;49m\u001b[39m\"\u001b[39;49m, batch, logs\u001b[39m=\u001b[39;49mlogs)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/callbacks.py:323\u001b[0m, in \u001b[0;36mCallbackList._call_batch_hook\u001b[0;34m(self, mode, hook, batch, logs)\u001b[0m\n\u001b[1;32m    321\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_batch_begin_hook(mode, batch, logs)\n\u001b[1;32m    322\u001b[0m \u001b[39melif\u001b[39;00m hook \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mend\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m--> 323\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_batch_end_hook(mode, batch, logs)\n\u001b[1;32m    324\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    325\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    326\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mUnrecognized hook: \u001b[39m\u001b[39m{\u001b[39;00mhook\u001b[39m}\u001b[39;00m\u001b[39m. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    327\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mExpected values are [\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mbegin\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m, \u001b[39m\u001b[39m\"\u001b[39m\u001b[39mend\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m]\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    328\u001b[0m     )\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/callbacks.py:346\u001b[0m, in \u001b[0;36mCallbackList._call_batch_end_hook\u001b[0;34m(self, mode, batch, logs)\u001b[0m\n\u001b[1;32m    343\u001b[0m     batch_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_batch_start_time\n\u001b[1;32m    344\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_batch_times\u001b[39m.\u001b[39mappend(batch_time)\n\u001b[0;32m--> 346\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_batch_hook_helper(hook_name, batch, logs)\n\u001b[1;32m    348\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_batch_times) \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_batches_for_timing_check:\n\u001b[1;32m    349\u001b[0m     end_hook_name \u001b[39m=\u001b[39m hook_name\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/callbacks.py:394\u001b[0m, in \u001b[0;36mCallbackList._call_batch_hook_helper\u001b[0;34m(self, hook_name, batch, logs)\u001b[0m\n\u001b[1;32m    392\u001b[0m \u001b[39mfor\u001b[39;00m callback \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallbacks:\n\u001b[1;32m    393\u001b[0m     hook \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(callback, hook_name)\n\u001b[0;32m--> 394\u001b[0m     hook(batch, logs)\n\u001b[1;32m    396\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_timing:\n\u001b[1;32m    397\u001b[0m     \u001b[39mif\u001b[39;00m hook_name \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_hook_times:\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/callbacks.py:1094\u001b[0m, in \u001b[0;36mProgbarLogger.on_train_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m   1093\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mon_train_batch_end\u001b[39m(\u001b[39mself\u001b[39m, batch, logs\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m-> 1094\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_batch_update_progbar(batch, logs)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/callbacks.py:1171\u001b[0m, in \u001b[0;36mProgbarLogger._batch_update_progbar\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m   1168\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m   1169\u001b[0m     \u001b[39m# Only block async when verbose = 1.\u001b[39;00m\n\u001b[1;32m   1170\u001b[0m     logs \u001b[39m=\u001b[39m tf_utils\u001b[39m.\u001b[39msync_to_numpy_or_python_type(logs)\n\u001b[0;32m-> 1171\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mprogbar\u001b[39m.\u001b[39;49mupdate(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mseen, \u001b[39mlist\u001b[39;49m(logs\u001b[39m.\u001b[39;49mitems()), finalize\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/utils/generic_utils.py:297\u001b[0m, in \u001b[0;36mProgbar.update\u001b[0;34m(self, current, values, finalize)\u001b[0m\n\u001b[1;32m    294\u001b[0m         info \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    296\u001b[0m     message \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m info\n\u001b[0;32m--> 297\u001b[0m     io_utils\u001b[39m.\u001b[39;49mprint_msg(message, line_break\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[1;32m    298\u001b[0m     message \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    300\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose \u001b[39m==\u001b[39m \u001b[39m2\u001b[39m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/utils/io_utils.py:80\u001b[0m, in \u001b[0;36mprint_msg\u001b[0;34m(message, line_break)\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     79\u001b[0m         sys\u001b[39m.\u001b[39mstdout\u001b[39m.\u001b[39mwrite(message)\n\u001b[0;32m---> 80\u001b[0m     sys\u001b[39m.\u001b[39;49mstdout\u001b[39m.\u001b[39;49mflush()\n\u001b[1;32m     81\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     82\u001b[0m     logging\u001b[39m.\u001b[39minfo(message)\n",
      "File \u001b[0;32m/envs/cv-env/lib/python3.8/site-packages/ipykernel/iostream.py:488\u001b[0m, in \u001b[0;36mOutStream.flush\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    486\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpub_thread\u001b[39m.\u001b[39mschedule(evt\u001b[39m.\u001b[39mset)\n\u001b[1;32m    487\u001b[0m     \u001b[39m# and give a timeout to avoid\u001b[39;00m\n\u001b[0;32m--> 488\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m evt\u001b[39m.\u001b[39;49mwait(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mflush_timeout):\n\u001b[1;32m    489\u001b[0m         \u001b[39m# write directly to __stderr__ instead of warning because\u001b[39;00m\n\u001b[1;32m    490\u001b[0m         \u001b[39m# if this is happening sys.stderr may be the problem.\u001b[39;00m\n\u001b[1;32m    491\u001b[0m         \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mIOStream.flush timed out\u001b[39m\u001b[39m\"\u001b[39m, file\u001b[39m=\u001b[39msys\u001b[39m.\u001b[39m__stderr__)\n\u001b[1;32m    492\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/lib/python3.8/threading.py:558\u001b[0m, in \u001b[0;36mEvent.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    556\u001b[0m signaled \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_flag\n\u001b[1;32m    557\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m signaled:\n\u001b[0;32m--> 558\u001b[0m     signaled \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_cond\u001b[39m.\u001b[39;49mwait(timeout)\n\u001b[1;32m    559\u001b[0m \u001b[39mreturn\u001b[39;00m signaled\n",
      "File \u001b[0;32m/usr/lib/python3.8/threading.py:306\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    304\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    305\u001b[0m     \u001b[39mif\u001b[39;00m timeout \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m--> 306\u001b[0m         gotit \u001b[39m=\u001b[39m waiter\u001b[39m.\u001b[39;49macquire(\u001b[39mTrue\u001b[39;49;00m, timeout)\n\u001b[1;32m    307\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    308\u001b[0m         gotit \u001b[39m=\u001b[39m waiter\u001b[39m.\u001b[39macquire(\u001b[39mFalse\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "simple_model.fit(train_df, validation_data=val_df, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-30 14:18:28.016614: W tensorflow/core/lib/png/png_io.cc:88] PNG warning: iCCP: known incorrect sRGB profile\n",
      "2022-11-30 14:18:28.016706: W tensorflow/core/lib/png/png_io.cc:88] PNG warning: iCCP: cHRM chunk does not match sRGB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "160/160 [==============================] - 2s 8ms/step - loss: 898.7798 - mape: 63.5435 - mae: 25.1694\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[898.77978515625, 63.54345703125, 25.169401168823242]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simple_model.evaluate(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGGBlock(K.Model):\n",
    "    def __init__(self, filters, kernel_size, repetitions, strides=2, pool_size=2):\n",
    "        super(VGGBlock, self).__init__()\n",
    "        self.filters = filters\n",
    "        self.kernel_size = kernel_size\n",
    "        # self.norm = K.layers.Rescaling(1./255)\n",
    "\n",
    "\n",
    "        self.repetitions = repetitions\n",
    "\n",
    "        for rep in range(self.repetitions):\n",
    "            vars(self)[f'conv2D_{rep}'] = K.layers.Conv2D(\n",
    "                filters=filters, kernel_size=kernel_size, activation='relu', padding='same'\n",
    "            )\n",
    "\n",
    "        # Defining the layers\n",
    "        # self.conv2D_0 = K.layers.Conv2D(\n",
    "        #     name='conv2D_0', \n",
    "        #     filters=self.filters, kernel_size=self.kernel_size, activation='relu', padding='same'\n",
    "        # )\n",
    "\n",
    "        self.max_pool = K.layers.MaxPool2D(\n",
    "            pool_size=pool_size, strides=strides\n",
    "        )\n",
    "        \n",
    "    def __call__(self, inputs, training=True):\n",
    "        x = self.conv2D_0(inputs)\n",
    "        x = self.max_pool(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGG(K.Model):\n",
    "    def __init__(self, activation):\n",
    "        super(VGG, self).__init__()\n",
    "        self.activation = K.activations.get(activation)\n",
    "        self.norm = K.layers.Rescaling(1./255)\n",
    "        self.block_0 = VGGBlock(64, 3, 2)\n",
    "        self.block_1 = VGGBlock(128, 3, 2)\n",
    "        self.block_2 = VGGBlock(256, 3, 3)\n",
    "        self.block_3 = VGGBlock(512, 3, 3)\n",
    "        self.block_4 = VGGBlock(512, 3, 3)\n",
    "        self.flatten = K.layers.Flatten()\n",
    "        self.dense = K.layers.Dense(units=4096, activation='relu')\n",
    "        self.predictor = K.layers.Dense(units=1, activation=self.activation)\n",
    "\n",
    "    def __call__(self, inputs, training=True):\n",
    "        x = self.norm(inputs)\n",
    "        x = self.block_0(x)\n",
    "        x = self.block_1(x)\n",
    "        x = self.block_2(x)\n",
    "        x = self.block_3(x)\n",
    "        x = self.block_4(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.dense(x)\n",
    "        x = self.predictor(x)\n",
    "        return x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg_model = VGG('linear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg_model.compile(\n",
    "    optimizer='adam', loss='mse', metrics=['mae', 'mape']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-30 17:11:47.360276: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:392] Filling up shuffle buffer (this may take a while): 7416 of 12800\n",
      "2022-11-30 17:11:54.747772: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:417] Shuffle buffer filled.\n",
      "2022-11-30 17:11:55.341778: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:428] Loaded cuDNN version 8100\n",
      "2022-11-30 17:11:56.748096: W tensorflow/tsl/framework/bfc_allocator.cc:360] Garbage collection: deallocate free memory regions (i.e., allocations) so that we can re-allocate a larger region to avoid OOM due to memory fragmentation. If you see this message frequently, you are running near the threshold of the available device memory and re-allocation may incur great performance overhead. You may try smaller batch sizes to observe the performance impact. Set TF_ENABLE_GPU_GARBAGE_COLLECTION=false if you'd like to disable this feature.\n",
      "2022-11-30 17:11:57.104783: I tensorflow/compiler/xla/service/service.cc:173] XLA service 0x1f3ca680 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2022-11-30 17:11:57.104835: I tensorflow/compiler/xla/service/service.cc:181]   StreamExecutor device (0): NVIDIA GeForce RTX 2080, Compute Capability 7.5\n",
      "2022-11-30 17:11:57.110269: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2022-11-30 17:11:57.217645: I tensorflow/compiler/jit/xla_compilation_cache.cc:477] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2560/2560 [==============================] - 123s 40ms/step - loss: 360287962136576.0000 - mae: 167774.2812 - mape: 6438675.0000 - val_loss: 238.8466 - val_mae: 11.9667 - val_mape: 26456886.0000\n",
      "Epoch 2/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-30 17:13:50.066028: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:392] Filling up shuffle buffer (this may take a while): 10841 of 12800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   3/2560 [..............................] - ETA: 1:28 - loss: 203.0317 - mae: 11.6412 - mape: 27.5319  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-30 17:13:51.897021: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:417] Shuffle buffer filled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2560/2560 [==============================] - 118s 41ms/step - loss: 360287962136576.0000 - mae: 167774.6406 - mape: 5723222.5000 - val_loss: 244.2556 - val_mae: 11.9323 - val_mape: 34.6041\n",
      "Epoch 3/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-30 17:15:47.643630: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:392] Filling up shuffle buffer (this may take a while): 12329 of 12800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   5/2560 [..............................] - ETA: 1:27 - loss: 88.9832 - mae: 8.3798 - mape: 28.5430 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-30 17:15:48.036950: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:417] Shuffle buffer filled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2560/2560 [==============================] - 109s 39ms/step - loss: 360287962136576.0000 - mae: 167783.2031 - mape: 4728774.5000 - val_loss: 270.3767 - val_mae: 12.9088 - val_mape: 25072536.0000\n",
      "Epoch 4/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-30 17:17:37.088246: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:392] Filling up shuffle buffer (this may take a while): 12595 of 12800\n",
      "2022-11-30 17:17:37.251903: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:417] Shuffle buffer filled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2560/2560 [==============================] - 110s 39ms/step - loss: 360287962136576.0000 - mae: 167791.7344 - mape: 3061576.2500 - val_loss: 712.0352 - val_mae: 20.8333 - val_mape: 64.6326\n",
      "Epoch 5/10\n",
      "2560/2560 [==============================] - 110s 39ms/step - loss: 360287962136576.0000 - mae: 167811.5625 - mape: 11008714.0000 - val_loss: 1854.8348 - val_mae: 34.0012 - val_mape: 101.1152\n",
      "Epoch 6/10\n",
      "2560/2560 [==============================] - 119s 43ms/step - loss: 360287928582144.0000 - mae: 167871.0781 - mape: 11860108.0000 - val_loss: 13576.5312 - val_mae: 92.7201 - val_mape: 277.8386\n",
      "Epoch 7/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-30 17:23:16.639183: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:392] Filling up shuffle buffer (this may take a while): 12665 of 12800\n",
      "2022-11-30 17:23:16.749523: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:417] Shuffle buffer filled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 183/2560 [=>............................] - ETA: 1:39 - loss: 12092.6641 - mae: 88.0639 - mape: 267.5892"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[69], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m vgg_model\u001b[39m.\u001b[39;49mfit(\n\u001b[1;32m      2\u001b[0m     train_df,\n\u001b[1;32m      3\u001b[0m     validation_data\u001b[39m=\u001b[39;49mval_df,\n\u001b[1;32m      4\u001b[0m     epochs \u001b[39m=\u001b[39;49m \u001b[39m10\u001b[39;49m\n\u001b[1;32m      5\u001b[0m )\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/engine/training.py:1656\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1654\u001b[0m logs \u001b[39m=\u001b[39m tmp_logs\n\u001b[1;32m   1655\u001b[0m end_step \u001b[39m=\u001b[39m step \u001b[39m+\u001b[39m data_handler\u001b[39m.\u001b[39mstep_increment\n\u001b[0;32m-> 1656\u001b[0m callbacks\u001b[39m.\u001b[39;49mon_train_batch_end(end_step, logs)\n\u001b[1;32m   1657\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstop_training:\n\u001b[1;32m   1658\u001b[0m     \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/callbacks.py:476\u001b[0m, in \u001b[0;36mCallbackList.on_train_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m    469\u001b[0m \u001b[39m\"\"\"Calls the `on_train_batch_end` methods of its callbacks.\u001b[39;00m\n\u001b[1;32m    470\u001b[0m \n\u001b[1;32m    471\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[1;32m    472\u001b[0m \u001b[39m    batch: Integer, index of batch within the current epoch.\u001b[39;00m\n\u001b[1;32m    473\u001b[0m \u001b[39m    logs: Dict. Aggregated metric results up until this batch.\u001b[39;00m\n\u001b[1;32m    474\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    475\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_should_call_train_batch_hooks:\n\u001b[0;32m--> 476\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_batch_hook(ModeKeys\u001b[39m.\u001b[39;49mTRAIN, \u001b[39m\"\u001b[39;49m\u001b[39mend\u001b[39;49m\u001b[39m\"\u001b[39;49m, batch, logs\u001b[39m=\u001b[39;49mlogs)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/callbacks.py:323\u001b[0m, in \u001b[0;36mCallbackList._call_batch_hook\u001b[0;34m(self, mode, hook, batch, logs)\u001b[0m\n\u001b[1;32m    321\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_batch_begin_hook(mode, batch, logs)\n\u001b[1;32m    322\u001b[0m \u001b[39melif\u001b[39;00m hook \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mend\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m--> 323\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_batch_end_hook(mode, batch, logs)\n\u001b[1;32m    324\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    325\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    326\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mUnrecognized hook: \u001b[39m\u001b[39m{\u001b[39;00mhook\u001b[39m}\u001b[39;00m\u001b[39m. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    327\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mExpected values are [\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mbegin\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m, \u001b[39m\u001b[39m\"\u001b[39m\u001b[39mend\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m]\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    328\u001b[0m     )\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/callbacks.py:346\u001b[0m, in \u001b[0;36mCallbackList._call_batch_end_hook\u001b[0;34m(self, mode, batch, logs)\u001b[0m\n\u001b[1;32m    343\u001b[0m     batch_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_batch_start_time\n\u001b[1;32m    344\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_batch_times\u001b[39m.\u001b[39mappend(batch_time)\n\u001b[0;32m--> 346\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_batch_hook_helper(hook_name, batch, logs)\n\u001b[1;32m    348\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_batch_times) \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_batches_for_timing_check:\n\u001b[1;32m    349\u001b[0m     end_hook_name \u001b[39m=\u001b[39m hook_name\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/callbacks.py:394\u001b[0m, in \u001b[0;36mCallbackList._call_batch_hook_helper\u001b[0;34m(self, hook_name, batch, logs)\u001b[0m\n\u001b[1;32m    392\u001b[0m \u001b[39mfor\u001b[39;00m callback \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallbacks:\n\u001b[1;32m    393\u001b[0m     hook \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(callback, hook_name)\n\u001b[0;32m--> 394\u001b[0m     hook(batch, logs)\n\u001b[1;32m    396\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_timing:\n\u001b[1;32m    397\u001b[0m     \u001b[39mif\u001b[39;00m hook_name \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_hook_times:\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/callbacks.py:1094\u001b[0m, in \u001b[0;36mProgbarLogger.on_train_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m   1093\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mon_train_batch_end\u001b[39m(\u001b[39mself\u001b[39m, batch, logs\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m-> 1094\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_batch_update_progbar(batch, logs)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/callbacks.py:1170\u001b[0m, in \u001b[0;36mProgbarLogger._batch_update_progbar\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m   1166\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mseen \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m add_seen\n\u001b[1;32m   1168\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m   1169\u001b[0m     \u001b[39m# Only block async when verbose = 1.\u001b[39;00m\n\u001b[0;32m-> 1170\u001b[0m     logs \u001b[39m=\u001b[39m tf_utils\u001b[39m.\u001b[39;49msync_to_numpy_or_python_type(logs)\n\u001b[1;32m   1171\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprogbar\u001b[39m.\u001b[39mupdate(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mseen, \u001b[39mlist\u001b[39m(logs\u001b[39m.\u001b[39mitems()), finalize\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/utils/tf_utils.py:665\u001b[0m, in \u001b[0;36msync_to_numpy_or_python_type\u001b[0;34m(tensors)\u001b[0m\n\u001b[1;32m    662\u001b[0m         \u001b[39mreturn\u001b[39;00m t\n\u001b[1;32m    663\u001b[0m     \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mitem() \u001b[39mif\u001b[39;00m np\u001b[39m.\u001b[39mndim(t) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m \u001b[39melse\u001b[39;00m t\n\u001b[0;32m--> 665\u001b[0m \u001b[39mreturn\u001b[39;00m tf\u001b[39m.\u001b[39;49mnest\u001b[39m.\u001b[39;49mmap_structure(_to_single_numpy_or_python_type, tensors)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/util/nest.py:917\u001b[0m, in \u001b[0;36mmap_structure\u001b[0;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[1;32m    913\u001b[0m flat_structure \u001b[39m=\u001b[39m (flatten(s, expand_composites) \u001b[39mfor\u001b[39;00m s \u001b[39min\u001b[39;00m structure)\n\u001b[1;32m    914\u001b[0m entries \u001b[39m=\u001b[39m \u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39mflat_structure)\n\u001b[1;32m    916\u001b[0m \u001b[39mreturn\u001b[39;00m pack_sequence_as(\n\u001b[0;32m--> 917\u001b[0m     structure[\u001b[39m0\u001b[39m], [func(\u001b[39m*\u001b[39mx) \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m entries],\n\u001b[1;32m    918\u001b[0m     expand_composites\u001b[39m=\u001b[39mexpand_composites)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/util/nest.py:917\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    913\u001b[0m flat_structure \u001b[39m=\u001b[39m (flatten(s, expand_composites) \u001b[39mfor\u001b[39;00m s \u001b[39min\u001b[39;00m structure)\n\u001b[1;32m    914\u001b[0m entries \u001b[39m=\u001b[39m \u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39mflat_structure)\n\u001b[1;32m    916\u001b[0m \u001b[39mreturn\u001b[39;00m pack_sequence_as(\n\u001b[0;32m--> 917\u001b[0m     structure[\u001b[39m0\u001b[39m], [func(\u001b[39m*\u001b[39;49mx) \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m entries],\n\u001b[1;32m    918\u001b[0m     expand_composites\u001b[39m=\u001b[39mexpand_composites)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/utils/tf_utils.py:658\u001b[0m, in \u001b[0;36msync_to_numpy_or_python_type.<locals>._to_single_numpy_or_python_type\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    655\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_to_single_numpy_or_python_type\u001b[39m(t):\n\u001b[1;32m    656\u001b[0m     \u001b[39m# Don't turn ragged or sparse tensors to NumPy.\u001b[39;00m\n\u001b[1;32m    657\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(t, tf\u001b[39m.\u001b[39mTensor):\n\u001b[0;32m--> 658\u001b[0m         t \u001b[39m=\u001b[39m t\u001b[39m.\u001b[39;49mnumpy()\n\u001b[1;32m    659\u001b[0m     \u001b[39m# Strings, ragged and sparse tensors don't have .item(). Return them\u001b[39;00m\n\u001b[1;32m    660\u001b[0m     \u001b[39m# as-is.\u001b[39;00m\n\u001b[1;32m    661\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(t, (np\u001b[39m.\u001b[39mndarray, np\u001b[39m.\u001b[39mgeneric)):\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py:1155\u001b[0m, in \u001b[0;36m_EagerTensorBase.numpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1132\u001b[0m \u001b[39m\"\"\"Copy of the contents of this Tensor into a NumPy array or scalar.\u001b[39;00m\n\u001b[1;32m   1133\u001b[0m \n\u001b[1;32m   1134\u001b[0m \u001b[39mUnlike NumPy arrays, Tensors are immutable, so this method has to copy\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1152\u001b[0m \u001b[39m    NumPy dtype.\u001b[39;00m\n\u001b[1;32m   1153\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1154\u001b[0m \u001b[39m# TODO(slebedev): Consider avoiding a copy for non-CPU or remote tensors.\u001b[39;00m\n\u001b[0;32m-> 1155\u001b[0m maybe_arr \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_numpy()  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m   1156\u001b[0m \u001b[39mreturn\u001b[39;00m maybe_arr\u001b[39m.\u001b[39mcopy() \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(maybe_arr, np\u001b[39m.\u001b[39mndarray) \u001b[39melse\u001b[39;00m maybe_arr\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py:1121\u001b[0m, in \u001b[0;36m_EagerTensorBase._numpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1119\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_numpy\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m   1120\u001b[0m   \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1121\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_numpy_internal()\n\u001b[1;32m   1122\u001b[0m   \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m   1123\u001b[0m     \u001b[39mraise\u001b[39;00m core\u001b[39m.\u001b[39m_status_to_exception(e) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "vgg_model.fit(\n",
    "    train_df,\n",
    "    validation_data=val_df,\n",
    "    epochs = 10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'vgg_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m vgg_model\u001b[39m.\u001b[39mevaluate(train_df)\n\u001b[1;32m      2\u001b[0m vgg_model\u001b[39m.\u001b[39mevaluate(val_df)\n\u001b[1;32m      3\u001b[0m vgg_model\u001b[39m.\u001b[39mevaluate(test_df)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'vgg_model' is not defined"
     ]
    }
   ],
   "source": [
    "vgg_model.evaluate(train_df)\n",
    "vgg_model.evaluate(val_df)\n",
    "vgg_model.evaluate(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-30 14:59:39.016692: W tensorflow/core/lib/png/png_io.cc:88] PNG warning: iCCP: known incorrect sRGB profile\n",
      "2022-11-30 14:59:39.181689: W tensorflow/core/lib/png/png_io.cc:88] PNG warning: iCCP: known incorrect sRGB profile\n",
      "2022-11-30 14:59:39.235726: W tensorflow/core/lib/png/png_io.cc:88] PNG warning: iCCP: known incorrect sRGB profile\n",
      "2022-11-30 14:59:39.435401: W tensorflow/core/lib/png/png_io.cc:88] PNG warning: iCCP: profile 'icc': 'RGB ': RGB color space not permitted on grayscale PNG\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 746ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-30 14:59:39.514409: W tensorflow/core/lib/png/png_io.cc:88] PNG warning: iCCP: known incorrect sRGB profile\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[38.877346],\n",
       "       [30.025137],\n",
       "       [36.559353],\n",
       "       [35.593525],\n",
       "       [36.950672]], dtype=float32)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def block(x, n_convs, filters, kernel_size, activation, pool_size, pool_stride, block_name):\n",
    "  '''\n",
    "  Defines a block in the VGG network.\n",
    "\n",
    "  Args:\n",
    "    x (tensor) -- input image\n",
    "    n_convs (int) -- number of convolution layers to append\n",
    "    filters (int) -- number of filters for the convolution layers\n",
    "    activation (string or object) -- activation to use in the convolution\n",
    "    pool_size (int) -- size of the pooling layer\n",
    "    pool_stride (int) -- stride of the pooling layer\n",
    "    block_name (string) -- name of the block\n",
    "\n",
    "  Returns:\n",
    "    tensor containing the max-pooled output of the convolutions\n",
    "  '''\n",
    "\n",
    "  for i in range(n_convs):\n",
    "      x = tf.keras.layers.Conv2D(filters=filters, kernel_size=kernel_size, activation=activation, padding='same', name=\"{}_conv{}\".format(block_name, i + 1))(x)\n",
    "    \n",
    "  x = tf.keras.layers.MaxPooling2D(pool_size=pool_size, strides=pool_stride, name=\"{}_pool{}\".format(block_name, i+1 ))(x)\n",
    "\n",
    "  return x\n",
    "\n",
    "\n",
    "def VGG_16(image_input):\n",
    "  '''\n",
    "  This function defines the VGG encoder.\n",
    "\n",
    "  Args:\n",
    "    image_input (tensor) - batch of images\n",
    "\n",
    "  Returns:\n",
    "    tuple of tensors - output of all encoder blocks plus the final convolution layer\n",
    "  '''\n",
    "\n",
    "  # create 5 blocks with increasing filters at each stage. \n",
    "  # you will save the output of each block (i.e. p1, p2, p3, p4, p5). \"p\" stands for the pooling layer.\n",
    "  x = block(image_input,n_convs=2, filters=64, kernel_size=(3,3), activation='relu',pool_size=(2,2), pool_stride=(2,2), block_name='block1')\n",
    "  p1= x\n",
    "\n",
    "  x = block(x,n_convs=2, filters=128, kernel_size=(3,3), activation='relu',pool_size=(2,2), pool_stride=(2,2), block_name='block2')\n",
    "  p2 = x\n",
    "\n",
    "  x = block(x,n_convs=3, filters=256, kernel_size=(3,3), activation='relu',pool_size=(2,2), pool_stride=(2,2), block_name='block3')\n",
    "  p3 = x\n",
    "\n",
    "  x = block(x,n_convs=3, filters=512, kernel_size=(3,3), activation='relu',pool_size=(2,2), pool_stride=(2,2), block_name='block4')\n",
    "  p4 = x\n",
    "\n",
    "  x = block(x,n_convs=3, filters=512, kernel_size=(3,3), activation='relu',pool_size=(2,2), pool_stride=(2,2), block_name='block5')\n",
    "  p5 = x\n",
    "\n",
    "  # create the vgg model\n",
    "  vgg  = tf.keras.Model(image_input , p5)\n",
    "\n",
    "  # load the pretrained weights you downloaded earlier\n",
    "  vgg.load_weights(vgg_weights_path) \n",
    "\n",
    "  # number of filters for the output convolutional layers\n",
    "  n = 4096\n",
    "\n",
    "  # our input images are 224x224 pixels so they will be downsampled to 7x7 after the pooling layers above.\n",
    "  # we can extract more features by chaining two more convolution layers.\n",
    "  c6 = tf.keras.layers.Conv2D( n , ( 7 , 7 ) , activation='relu' , padding='same', name=\"conv6\")(p5)\n",
    "  c7 = tf.keras.layers.Conv2D( n , ( 1 , 1 ) , activation='relu' , padding='same', name=\"conv7\")(c6)\n",
    "\n",
    "  # return the outputs at each stage. you will only need two of these in this particular exercise \n",
    "  # but we included it all in case you want to experiment with other types of decoders.\n",
    "  return (p1, p2, p3, p4, c7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Regularization in a NN\n",
    "# J(wi, bi, ...) = 1 / m sum(loss(y_hat_i, y_i)) + [ L2normReg(w[l]) for l in layers ],  \n",
    "# Frobenius norm (not called L2 for maths reason (linear algrebra)),   \n",
    "# dw gets a new term = lambda / m.  \n",
    "# Weight Decay: Multiplying the weight matrix by a (1 -alpha * lambda / 2m)y\n",
    "\n",
    "# the term lambda forces W close to zero which makes some neurons have a smaller effect in the prediction. L1 norm would actually make them zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('cv-env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "88de0083f64d3083508852baa7dae69acbad25dc7a9be700ce04fff49d0b1750"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
